# ml-inference-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-inference
  labels:
    app: ml-inference
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ml-inference
  template:
    metadata:
      labels:
        app: ml-inference
    spec:
      containers:
        - name: ml-inference
          image: 192.168.5.71:5000/ml-inference-image:latest
          ports:
            - containerPort: 5000
---
# ml-inference-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: ml-inference
spec:
  ports:
    - port: 5000
      targetPort: 5000
  selector:
    app: ml-inference
